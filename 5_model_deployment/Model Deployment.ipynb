{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb9db4f",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "## 4.1 Overview\n",
    "\n",
    "The fourth week of Machine Learning Zoomcamp is about different metrics to evaluate a binary classifier. These measures include accuracy, confusion table, precision, recall, ROC curves(TPR, FRP, random model, and ideal model), AUROC, and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c23f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5e44399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8b186f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customer_data.csv')\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "for cell in categorical_columns:\n",
    "    df[cell] = df[cell].str.lower().str.replace(' ', '_')\n",
    "    \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b869d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train.churn.values\n",
    "y_val = df_val.churn.values\n",
    "y_test = df_val.churn.values\n",
    "\n",
    "del df_train['churn']\n",
    "del df_val['churn']\n",
    "del df_test['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be142dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents', \n",
    "        'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', \n",
    "        'techsupport','streamingtv', 'streamingmovies', \n",
    "        'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52fc0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdc4f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement, we have to create a train function\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "    \n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0341e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "    \n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77a665fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "492fba3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.841 +- 0.008\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "C = 1.0\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    "\n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d2171d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8435682269548084,\n",
       " 0.8458337471858032,\n",
       " 0.8311780052177403,\n",
       " 0.8301724275756219,\n",
       " 0.8517774779580183]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5035238731962783"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train our model once more on C=1.0\n",
    "\n",
    "\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc\n",
    "\n",
    "# the result ought to be 0.8572386167896259"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa054a",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "## 5.1 Intro/Session Overview\n",
    "\n",
    "In This session we talked about the earlier model we made in chapter 3 for churn prediction. This chapter contains the deployment of the model. If we want to use the model to predict new values without running the code, There's a way to do this. The way to use the model in different machines without running the code, is to deploy the model in a server (run the code and make the model). After deploying the code in a machine used as server we can make some endpoints (using api's) to connect from another machine to the server and predict values.\n",
    "\n",
    "To deploy the model in a server there are some steps:\n",
    "\n",
    "- After training the model save it, to use it for making predictions in future (session 02-pickle).\n",
    "- Make the API endpoints in order to request predictions. (session 03-flask-intro and 04-flask-deployment)\n",
    "- Some other server deployment options (sessions 5 to 9)\n",
    "\n",
    "We train a model in jupyter notebook, then we use the model by saving it to a file(model.bin). We want to load the file from a service e.g. churn service and the model will be inside the service. \n",
    "\n",
    "Say we have another service such as a marketing service that contain all the users. The marketing service can send a request to the churn service with info about the user, then the churn service sends back prediciton to the marketing service and based on the prediction received, the marketing service can decide whether they want to send promotional email with discount prices.\n",
    "\n",
    "We put the churn prediction model into a web service using flask (a framework for creating web services in python). We want to wrap the service in such a way that it does not interfere with other services that we have in our machine. What we want to do is to create a special environment for our python dependencies - for this, we will use `pipenv`. We have another layer for system dependencies, then deploy the container to the cloud\n",
    "\n",
    "![deployment_image](deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f5221",
   "metadata": {},
   "source": [
    "## 5.2 Saving and Loading the Model\n",
    "\n",
    "- Saving the model to pickle\n",
    "- Loading the model to pickle\n",
    "- Turning our notebook into a python script\n",
    "\n",
    "\n",
    "### Saving the Model\n",
    "To save the model we made before there is an option using the pickle library:\n",
    "- First install the library with the command pip install pickle-mixin if you don't have it.\n",
    "- After training the model and being the model ready for prediction process use this code to save the model for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb1a49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle for saving python objects\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3bda070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_file = 'model_C=%s.bin' % C\n",
    "output_file = f'model_C={C}.bin'\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "250829bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a file and write to it\n",
    "# f_out = open(output_file, 'wb')\n",
    "\n",
    "# # save our model\n",
    "# pickle.dump((dv, model), f_out)\n",
    "\n",
    "# # close the file\n",
    "# f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea9dd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a methodology that ensures that the file is always closed\n",
    "# instead of the cell above, we can rewrite:\n",
    "\n",
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)\n",
    "    # do stuff\n",
    "# here, once outside the 'with' statement, the file closes\n",
    "# and we can do other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the code above we'll making a binary file named model.bin and writing the dict_vectorizer for one hot encoding and model as array in it. (We will save it as binary in case it wouldn't be readable by humans)\n",
    "- To be able to use the model in future without running the code, We need to open the binary file we saved before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee894a7d",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "\n",
    "We restarted the kernel so that we can mimick loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c75aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb2a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model_C=10.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "199a4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read the file here\n",
    "\n",
    "with open(model_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unpacking the model and the dict_vectorizer, We're able to again predict for new input values without training a new model by re-running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aef553e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DictVectorizer(sparse=False), LogisticRegression(max_iter=1000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506ddd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a new customer informations\n",
    "customer = {\n",
    "  'customerid': '8879-zkjof',\n",
    "  'gender': 'female',\n",
    "  'seniorcitizen': 0,\n",
    "  'partner': 'no',\n",
    "  'dependents': 'no',\n",
    "  'tenure': 41,\n",
    "  'phoneservice': 'yes',\n",
    "  'multiplelines': 'no',\n",
    "  'internetservice': 'dsl',\n",
    "  'onlinesecurity': 'yes',\n",
    "  'onlinebackup': 'no',\n",
    "  'deviceprotection': 'yes',\n",
    "  'techsupport': 'yes',\n",
    "  'streamingtv': 'yes',\n",
    "  'streamingmovies': 'yes',\n",
    "  'contract': 'one_year',\n",
    "  'paperlessbilling': 'yes',\n",
    "  'paymentmethod': 'bank_transfer_(automatic)',\n",
    "  'monthlycharges': 79.85,\n",
    "  'totalcharges': 3320.75\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25809d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06224295541445037"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to turn the customer above into a feature matrix\n",
    "X = dv.transform([customer])\n",
    "model.predict_proba(X)[0, 1]\n",
    "# the result shows that the customer is going to churn for the value of\n",
    "# 0.6363584152715288 cos the result from the cell is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cddf8c",
   "metadata": {},
   "source": [
    "It is not convenient to train a model from jupyter notebook. The best way is to create a python file/script that does the training. We do that by downloading the notebook as a python file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aeabbf",
   "metadata": {},
   "source": [
    "## 5.3 Web Services: Introduction to Flask\n",
    "\n",
    "Web services are services you communicate with over a network using some protocol. We can use flask for implementing it.\n",
    "\n",
    "In this session, we create a simple service where user sends a `ping` address and it responds with a `pong`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4339e",
   "metadata": {},
   "source": [
    "## 5.3 Web services: introduction to Flask\n",
    "- A web service is a method used to communicate between electronic devices.\n",
    "- There are some methods in web services we can use it to satisfy our problems. Here below we would list some.\n",
    "    - GET: GET is a method used to retrieve files, For example when we are searching for a cat image in google we are actually requesting cat images with GET method.\n",
    "    - POST: POST is the second common method used in web services. For example in a sign up process, when we are submiting our name, username, passwords, etc we are posting our data to a server that is using the web service. (Note that there is no specification where the data goes)\n",
    "    - PUT: PUT is same as POST but we are specifying where the data is going to.\n",
    "    - DELETE: DELETE is a method that is used to request to delete some data from the server.\n",
    "- To create a simple web service, there are plenty libraries available in every language. Here we would like to introduce Flask library in python.\n",
    "    - If you haven't installed the library just try installing it with the code pip install Flask\n",
    "    - To create a simple web service just create the code in a python file, `ping.py` inside the parent folder of this ml zoomcamp notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Serving the Churn Model with Flask\n",
    "\n",
    "In this session we talked about implementing the functionality of prediction to our churn web service and how to make it usable in development environment.\n",
    "\n",
    "- To make the web service predict the churn value for each customer we must modify the code in session 3 with the code we had in previous chapters. Below we can see how the code works in order to predict the churn value.\n",
    "- In order to predict we need to first load the previous saved model and use a prediction function in a special route.\n",
    "To load the previous saved model we use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c540dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('/churn-model.bin', 'rb') as f_in:\n",
    "#   dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had earlier to predict a value for a customer we need a function like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_single(customer, dv, model):\n",
    "#   X = dv.transform([customer])  ## apply the one-hot encoding feature to the customer data \n",
    "#   y_pred = model.predict_proba(X)[:, 1]\n",
    "#   return y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then at last we make the final function used for creating the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])  ## in order to send the customer information we need to post its data.\n",
    "# def predict():\n",
    "# customer = request.get_json()  ## web services work best with json frame, So after the user post its data in json format we need to access the body of json.\n",
    "\n",
    "# prediction = predict_single(customer, dv, model)\n",
    "# churn = prediction >= 0.5\n",
    "\n",
    "# result = {\n",
    "#     'churn_probability': float(prediction), ## we need to cast numpy float type to python native float type\n",
    "#     'churn': bool(churn),  ## same as the line above, casting the value using bool method\n",
    "# }\n",
    "\n",
    "# return jsonify(result)  ## send back the data in json format to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last run your code. To see the result we can't use a simple request in web browser, because we are expecting a POST request in our app. We can run the code below to post customer data as json and see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## a new customer informations\n",
    "# customer = {\n",
    "#   'customerid': '8879-zkjof',\n",
    "#   'gender': 'female',\n",
    "#   'seniorcitizen': 0,\n",
    "#   'partner': 'no',\n",
    "#   'dependents': 'no',\n",
    "#   'tenure': 41,\n",
    "#   'phoneservice': 'yes',\n",
    "#   'multiplelines': 'no',\n",
    "#   'internetservice': 'dsl',\n",
    "#   'onlinesecurity': 'yes',\n",
    "#   'onlinebackup': 'no',\n",
    "#   'deviceprotection': 'yes',\n",
    "#   'techsupport': 'yes',\n",
    "#   'streamingtv': 'yes',\n",
    "#   'streamingmovies': 'yes',\n",
    "#   'contract': 'one_year',\n",
    "#   'paperlessbilling': 'yes',\n",
    "#   'paymentmethod': 'bank_transfer_(automatic)',\n",
    "#   'monthlycharges': 79.85,\n",
    "#   'totalcharges': 3320.75\n",
    "# }\n",
    "# import requests ## to use the POST method we use a library named requests\n",
    "# url = 'http://localhost:9696/predict' ## this is the route we made for prediction\n",
    "# response = requests.post(url, json=customer) ## post the customer information in json format\n",
    "# result = response.json() ## get the server response\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Until here we saw how we made a simple web server that predicts the churn value for every user. When you run your app you will see a warning that it is not a WGSI server and not suitable for production environmnets. To fix this issue and run this as a production server there are plenty of ways available.\n",
    "\n",
    "    - One way to create a WSGI server is to use gunicorn. To install it use the command `pip install gunicorn`, And to run the WGSI server you can simply run it with the command `gunicorn --bind 0.0.0.0:9696 churn:app`. Note that in churn:app the name churn is the name we set for our the file containing the code `app = Flask('churn')`(for example: churn.py), You may need to change it to whatever you named your Flask app file.\n",
    "    - Windows users may not be able to use gunicorn library because windows system do not support some dependecies of the library. So to be able to run this on a windows machine, there is an alternative library waitress and to install it just use the command `pip install waitress`.\n",
    "    - to run the waitress wgsi server use the command `waitress-serve --listen=0.0.0.0:9696 churn:app`.\n",
    "    - To test it just you can run the code above and the results is the same.\n",
    "- So until here you were able to make a production server that predict the churn value for new customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Python virtual environment: Pipenv\n",
    "\n",
    "In this session we're going to make virtual environment for our project. So Let's start this session to get to know what is a virtual environment and how to make it.\n",
    "\n",
    "- Every time we're running a file from a directory we're using the executive files from a global directory. For example when we install python on our machine the executable files that are able to run our codes will go to somewhere like */home/username/python/bin/* for example the pip command may go to */home/username/python/bin/pip*.\n",
    "- Sometimes the versions of libraries conflict (the project may not run or get into massive errors). For example we have an old project that uses sklearn library with the version of 0.24.1 and now we want to run it using sklearn version 1.0.0. We may get into errors because of the version conflict.\n",
    "    - To solve the conflict we can make virtual environments. Virtual environment is something that can seperate the libraries installed in our system and the libraries with specified version we want our project to run with. There are a lot of ways to create a virtual environments. One way we are going to use is using a library named pipenv.\n",
    "    - pipenv is a library that can create a virutal environment. To install this library just use the classic method `pip install pipenv`.\n",
    "    - After installing pipenv we must to install the libraries we want for our project in the new virtual environment. It's really easy, Just use the command pipenv instead of pip. `pipenv install numpy sklearn==0.24.1 flask`. With this command we installed the libraries we want for our project.\n",
    "    - Note that using the pipenv command we made two files named *Pipfile* and *Pipfile.lock*. If we look at this files closely we can see that in Pipfile the libraries we installed are named. If we specified the library name, it's also specified in Pipfile.\n",
    "    - In *Pipfile.lock* we can see that each library with it's installed version is named and a hash file is there to reproduce if we move the environment to another machine.\n",
    "    - If we want to run the project in another machine, we can easily installed the libraries we want with the command `pipenv install`. This command will look into *Pipfile* and *Pipfile*.lock to install the libraries with specified version.\n",
    "    - After installing the required libraries we can run the project in the virtual environment with `pipenv shell` command. This will go to the virtual environment's shell and then any command we execute will use the virtual environment's libraries.\n",
    "Installing and using the libraries such as gunicorn is the same as the last session.\n",
    "Until here we made a virtual environment for our libraries with a required specified version. To seperate this environment more, such as making gunicorn be able to run in windows machines we need another way. The other way is using Docker. Docker allows us to seperate everything more than before and make any project able to run on any machine that support Docker smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Environment management: Docker\n",
    "\n",
    "### Installing Docker\n",
    "\n",
    "To isolate more our project file from our system machine, there is an option named Docker. With Docker you are able to pack all your project is a system that you want and run it in any system machine. For example if you want Ubuntu 20.4 you can have it in a mac or windows machine or other operating systems.\n",
    "To get started with Docker for the churn prediction project you can follow the instructions below.\n",
    "\n",
    "### Ubuntu\n",
    "\n",
    "`sudo apt-get install docker.io`\n",
    "\n",
    "To run docker without sudo, follow [this instruction](https://docs.docker.com/engine/install/linux-postinstall/).\n",
    "\n",
    "### Windows\n",
    "To install the Docker you can just follow the instruction by Andrew Lock in this link: (https://andrewlock.net/installing-docker-desktop-for-windows/)\n",
    "\n",
    "### MacOS\n",
    "Follow the steps in the [Docker docs](https://docs.docker.com/desktop/install/mac-install/).\n",
    "\n",
    "### Notes\n",
    "- Once our project was packed in a Docker container, we're able to run our project on any machine.\n",
    "- First we have to make a Docker image. In Docker image file there are settings and dependecies we have in our project. To find Docker images that you need you can simply search the [Docker](https://hub.docker.com/search?type=image&q=) website.\n",
    "\n",
    "Here a Dockerfile (There should be no comments in Dockerfile, so remove the comments when you copy)\n",
    "\n",
    "        # First install the python 3.8, the slim version uses less space\n",
    "        FROM python:3.8.12-slim - `docker run -it --rm python:3.8.18-slim`. If you want to be able to access the terminal, `docker run -it --rm --entrypoint=bash python:3.8.18-slim`\n",
    "\n",
    "        # Install pipenv library in Docker \n",
    "        RUN pip install pipenv\n",
    "\n",
    "        # create a directory in Docker named app and we're using it as work directory \n",
    "        WORKDIR /app                                                                \n",
    "        # Copy the Pip files into our working derectory \n",
    "        COPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]\n",
    "\n",
    "        # Build the image - e.g. `docker build -t zoomcamp-test .`, then run it - `docker run -it --rm --entrypoint=bash zoomcamp-test`\n",
    "\n",
    "        # install the pipenv dependencies for the project and deploy them.\n",
    "        RUN pipenv install --deploy --system\n",
    "\n",
    "        # Copy any python files and the model we had to the working directory of Docker \n",
    "        COPY [\"*.py\", \"churn-model.bin\", \"./\"]\n",
    "\n",
    "        # We need to expose the 9696 port because we're not able to communicate with Docker outside it\n",
    "        EXPOSE 9696\n",
    "\n",
    "        # If we run the Docker image, we want our churn app to be running\n",
    "        ENTRYPOINT [\"gunicorn\", \"--bind\", \"0.0.0.0:9696\", \"churn_serving:app\"]\n",
    "\n",
    "#### Summary\n",
    "* Running a python image with docker\n",
    "* Dockerfile\n",
    "* Building a docker image\n",
    "* Runnning a docker image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
